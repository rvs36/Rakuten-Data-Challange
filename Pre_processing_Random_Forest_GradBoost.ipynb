{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLzInlLc14Je"
   },
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://drive.google.com/uc?id=1PYaJj1JKVpS9PD6spplswLb0hegiSu6P'\n",
    "\n",
    "# output = '/home/jupyter/Ensemble/X_train_update.csv'\n",
    "# gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Be0YpRz008t2"
   },
   "outputs": [],
   "source": [
    "# Import training and testing data \n",
    "X_train = pd.read_csv('/home/jupyter/Ensemble/X_train_update.csv', index_col=0)\n",
    "\n",
    "X_test = pd.read_csv('/home/jupyter/Ensemble/X_test_update.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('/home/jupyter/Ensemble/Y_train_CVw08PX.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1583875659599,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "0663ouQI2R_r",
    "outputId": "ee78918b-6f18-40cc-9548-6a395fcafe56"
   },
   "outputs": [],
   "source": [
    "# visualizing class distribution \n",
    "\n",
    "# Number of images per class label\n",
    "y_train.prdtypecode.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1583875664185,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "RIIM8dBB3TXC",
    "outputId": "8fdf619d-e6c9-4f71-f70e-d3b6d1b74729"
   },
   "outputs": [],
   "source": [
    "#visualize the datframes\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the datframes\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1583875668060,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "GzSUWL5yHv73",
    "outputId": "48a9dac8-f41c-4882-a089-8aceb6a3ccea"
   },
   "outputs": [],
   "source": [
    "#visualize the datframes\n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbT6dgmgHAKh"
   },
   "outputs": [],
   "source": [
    "# dropping the irrelevant columns from the data frames\n",
    "\n",
    "del X_train['description'], X_train['productid'], X_train['imageid']\n",
    "\n",
    "del X_test['description'], X_test['productid'], X_test['imageid']\n",
    "\n",
    "#renaming the column in y train \n",
    "y_train = y_train.rename(columns={\"prdtypecode\": \"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Us-Hjt0IQ9C"
   },
   "source": [
    "Now for all designation column in, we :\n",
    "\n",
    "1. remove the accents - done\n",
    "2. make to lowercase - done\n",
    "3. remove the special char like degree or phi - done\n",
    "#To be discussed - may we should remove the numbers as well\n",
    "4. Tokenize - Done\n",
    "5. Handling Punctuations\n",
    "4. Stop work removal  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqaiTuP5f-zk"
   },
   "outputs": [],
   "source": [
    "# Function to normalize\n",
    "def normalize_accent(string):\n",
    "    string = string.replace('á', 'a')\n",
    "    string = string.replace('â', 'a')\n",
    "\n",
    "    string = string.replace('é', 'e')\n",
    "    string = string.replace('è', 'e')\n",
    "    string = string.replace('ê', 'e')\n",
    "    string = string.replace('ë', 'e')\n",
    "\n",
    "    string = string.replace('î', 'i')\n",
    "    string = string.replace('ï', 'i')\n",
    "\n",
    "    string = string.replace('ö', 'o')\n",
    "    string = string.replace('ô', 'o')\n",
    "    string = string.replace('ò', 'o')\n",
    "    string = string.replace('ó', 'o')\n",
    "\n",
    "    string = string.replace('ù', 'u')\n",
    "    string = string.replace('û', 'u')\n",
    "    string = string.replace('ü', 'u')\n",
    "\n",
    "    string = string.replace('ç', 'c')\n",
    "\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVepJcw7jmCJ"
   },
   "outputs": [],
   "source": [
    "# function to tokenize\n",
    "def raw_to_tokens(raw_string, spacy_nlp):\n",
    "    # Write code for lower-casing\n",
    "    string = raw_string.lower()\n",
    "    \n",
    "    # Write code to normalize the accents\n",
    "    string = normalize_accent(string)\n",
    "        \n",
    "    # Write code to tokenize\n",
    "    spacy_tokens = spacy_nlp(string)\n",
    "        \n",
    "    # Write code to remove punctuation tokens and create string tokens\n",
    "    string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct if not token.is_stop]\n",
    "    \n",
    "    # Write code to join the tokens back into a single string\n",
    "    clean_string = \" \".join(string_tokens)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azhmkjR_gDAv"
   },
   "outputs": [],
   "source": [
    "# # remove the accents\n",
    "# X_train['designation_deaccent'] = X_train['designation'].apply(lambda x: normalize_accent(x))\n",
    "# X_test['designation_deaccent'] = X_test['designation'].apply(lambda x: normalize_accent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU6_tR0pg5d-"
   },
   "outputs": [],
   "source": [
    "# # make to lowercase\n",
    "# X_train['designation_lower'] = X_train['designation_deaccent'].apply(lambda x: x.lower())\n",
    "# X_test['designation_lower'] = X_test['designation_deaccent'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fouzoCTuiLcD"
   },
   "outputs": [],
   "source": [
    "# remove the special char like degree or phi \n",
    "X_train['designation_sp'] = X_train['designation'].apply(lambda x: re.sub('\\W+',\" \", x ))\n",
    "X_test['designation_sp'] = X_test['designation'].apply(lambda x: re.sub('\\W+',\" \", x ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reomve numbers\n",
    "X_train['designation_number'] = X_train['designation_sp'].str.replace('\\d+', '')\n",
    "X_test['designation_number'] = X_test['designation_sp'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13350,
     "status": "ok",
     "timestamp": 1583875775230,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "Nsf2XA_IWWo9",
    "outputId": "b8a3201a-c54c-413d-87dd-03caf85b3102"
   },
   "outputs": [],
   "source": [
    "# import and load spacy and download the french package\n",
    "\n",
    "# !python -m spacy download fr\n",
    "\n",
    "\n",
    "# Load spaCy for french\n",
    "spacy_nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWHtztF3j4Vu"
   },
   "outputs": [],
   "source": [
    "# tokenize the train file\n",
    "X_train['designation_token'] = X_train['designation_number'].apply(lambda x: raw_to_tokens(x, spacy_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the test file\n",
    "X_test['designation_token'] = X_test['designation_number'].apply(lambda x: raw_to_tokens(x, spacy_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BHQ9w1mVpja"
   },
   "outputs": [],
   "source": [
    "# save to a csv for future use\n",
    "X_train.to_csv('/home/jupyter/Ensemble/X_train_mid1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv for future use\n",
    "X_test.to_csv('/home/jupyter/Ensemble/X_test_mid1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPKdS4NmVVv4"
   },
   "source": [
    "## Since the above step takes a lot of time to run, the dataframe is saved as a csv after it and then, the csv file can be loaded as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('/home/jupyter/Ensemble/X_train_mid1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('/home/jupyter/Ensemble/X_test_mid1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.read_csv('/home/jupyter/Ensemble/Y_train_CVw08PX.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list_train = X_train['designation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list_test = X_test['designation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [desc_list_train, desc_list_test]\n",
    "consolidated_desc = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Write code to create a TfidfVectorizer object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Write code to vectorize the sample text\n",
    "X_tfidf_sample = tfidf.fit_transform(consolidated_desc.astype('U'))\n",
    "\n",
    "print(\"Shape of the TF-IDF Matrix:\")\n",
    "print(X_tfidf_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinal step to print details about the matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X_tfidf_sample.todense())\n",
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now save the matrix and import it for our use later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7c8710a9326d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jupyter/Ensemble/X_consolidated_without_numbers.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tfidf_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tfidf_sample' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"/home/jupyter/Ensemble/X_consolidated_without_numbers.npz\", X_tfidf_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the matrix\n",
    "from scipy import sparse\n",
    "X_tfidf_sample = sparse.load_npz(\"/home/jupyter/Ensemble/X_consolidated_without_numbers.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF Matrix:\n",
      "(98728, 64055)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the TF-IDF Matrix:\")\n",
    "print(X_tfidf_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tfidf_sample[:84916, : ]\n",
    "X_test = X_tfidf_sample[84916:, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 64055)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 64055)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  # use decision tree as base estimator\n",
    "bagging = BaggingClassifier(None, 20, 0.5, 0.5)\n",
    "\n",
    "bagging.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_numbers = pd.DataFrame(bagging.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_numbers.to_csv('without_numbers_and_SC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I will try random forest and boosting to get the best accuracy resluts and hence extrapolate the one on the entire dataset\n",
    "\n",
    "# 1. Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we deine our new training data, consisting of just 25% of the total data\n",
    "\n",
    "# X_train_grid = X_tfidf_sample[:21234, : ]\n",
    "# X_test_grid = X_tfidf_sample[84916:, : ]\n",
    "# y_train_grid = y_train.iloc[:21234, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 61, 72, 83, 94, 105, 116, 127, 138, 150], 'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 200, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(50, 150, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 95.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 31, 52, 73, 94, 115,\n",
       "                                                      136, 157, 178, 200,\n",
       "                                                      None],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [50, 61, 72, 83, 94,\n",
       "                                                         105, 116, 127, 138,\n",
       "                                                         150]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                               n_iter = 20, cv = 3, verbose=1, random_state=42, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': 178,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([140.40719144,   9.3549188 ,  30.88645101,  89.83845814,\n",
       "        124.89654175, 132.02254089, 112.65080953,  59.61499405,\n",
       "        126.19765369,  11.68609269,  43.94501336,   9.87197081,\n",
       "        208.77011077,  47.4842279 , 136.63955665, 217.78881844,\n",
       "        116.13031642,  53.44487143, 153.79977632,  59.82852991]),\n",
       " 'std_fit_time': array([0.74994576, 0.02510521, 0.09757134, 0.14512144, 0.68574227,\n",
       "        0.77118391, 1.40720057, 0.20873075, 0.57440172, 0.04582493,\n",
       "        0.0431229 , 0.04347796, 0.44130817, 0.10016098, 0.60393185,\n",
       "        1.26997071, 0.22188391, 0.12085492, 1.1763318 , 0.2450794 ]),\n",
       " 'mean_score_time': array([1.40923389, 0.49251469, 0.56308293, 0.80073396, 0.94220487,\n",
       "        1.15408246, 1.27679094, 0.97044357, 1.11701147, 0.62109629,\n",
       "        0.69996262, 0.36167288, 1.79140059, 0.96000854, 1.19544021,\n",
       "        1.59905386, 1.25843318, 0.80574711, 2.2896688 , 0.89618556]),\n",
       " 'std_score_time': array([0.00714354, 0.00659295, 0.00529637, 0.00347318, 0.01024145,\n",
       "        0.00583836, 0.00762838, 0.00969911, 0.00317832, 0.00960188,\n",
       "        0.00850318, 0.00556511, 0.02104816, 0.00599119, 0.02180475,\n",
       "        0.00629545, 0.01986216, 0.00695715, 0.0194269 , 0.01051677]),\n",
       " 'param_n_estimators': masked_array(data=[138, 116, 61, 61, 50, 83, 94, 94, 83, 150, 116, 83,\n",
       "                    150, 127, 94, 116, 138, 127, 94, 72],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[5, 10, 10, 2, 10, 10, 5, 10, 2, 2, 2, 10, 10, 2, 5, 5,\n",
       "                    10, 10, 10, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[4, 2, 4, 4, 1, 4, 1, 2, 2, 4, 2, 4, 1, 4, 4, 1, 2, 1,\n",
       "                    4, 2],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[94, 10, 73, 157, 178, 178, 115, 94, 157, 10, 31, 10,\n",
       "                    94, 52, 157, 115, 73, 31, None, 136],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_bootstrap': masked_array(data=[False, True, True, False, False, False, True, True,\n",
       "                    False, True, False, False, False, True, False, False,\n",
       "                    False, False, True, True],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 138,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 94,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 116,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 10,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 61,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 73,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 61,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 157,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 50,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_depth': 178,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 83,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 178,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 94,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_depth': 115,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 94,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 94,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 83,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 157,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 10,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 116,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 31,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 83,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 10,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_depth': 94,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 127,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 52,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 94,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': 157,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 116,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_depth': 115,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 138,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 73,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 127,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_depth': 31,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 94,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_depth': None,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 72,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_depth': 136,\n",
       "   'bootstrap': True}],\n",
       " 'split0_test_score': array([0.58137938, 0.22265443, 0.5607815 , 0.63644345, 0.72756888,\n",
       "        0.64728671, 0.69796142, 0.59116562, 0.65363385, 0.21060464,\n",
       "        0.46655387, 0.23150668, 0.66616219, 0.53326972, 0.63876773,\n",
       "        0.70634905, 0.58314898, 0.48550214, 0.70513257, 0.63946765]),\n",
       " 'split1_test_score': array([0.58492507, 0.20932059, 0.5451202 , 0.63251308, 0.7218409 ,\n",
       "        0.63454299, 0.69166652, 0.58864645, 0.64435653, 0.20703029,\n",
       "        0.46646771, 0.20972605, 0.65381796, 0.52256672, 0.63028189,\n",
       "        0.68989314, 0.57892997, 0.49414396, 0.69453865, 0.6314198 ]),\n",
       " 'split2_test_score': array([0.57712722, 0.20938027, 0.55030694, 0.62640332, 0.72013448,\n",
       "        0.63576179, 0.68756865, 0.58599211, 0.64210193, 0.21046792,\n",
       "        0.44875868, 0.23177688, 0.65242411, 0.52063038, 0.62706739,\n",
       "        0.67513728, 0.56941538, 0.48201987, 0.69743182, 0.62453455]),\n",
       " 'mean_test_score': array([0.58114389, 0.2137851 , 0.55206955, 0.63178662, 0.72318142,\n",
       "        0.63919717, 0.69239886, 0.5886014 , 0.64669743, 0.20936762,\n",
       "        0.46059342, 0.22433653, 0.65746808, 0.52548894, 0.632039  ,\n",
       "        0.69045982, 0.57716477, 0.48722199, 0.69903434, 0.63180733]),\n",
       " 'std_test_score': array([0.00318781, 0.00627162, 0.00651404, 0.00413093, 0.00317966,\n",
       "        0.00574177, 0.00427432, 0.00211232, 0.0049904 , 0.00165368,\n",
       "        0.0083685 , 0.01033176, 0.00617394, 0.00555835, 0.00493559,\n",
       "        0.01274845, 0.00574398, 0.00509685, 0.00447093, 0.00610257]),\n",
       " 'rank_test_score': array([12, 19, 14, 10,  1,  7,  3, 11,  6, 20, 17, 18,  5, 15,  8,  4, 13,\n",
       "        16,  2,  9], dtype=int32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_boosting = pd.DataFrame(rf_random.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_boosting.to_csv('rand_boosting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model based on random search of hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf_rand = RandomForestClassifier(n_estimators = 167, min_samples_split = 5, min_samples_leaf = 1, max_depth = None ,max_features = 'auto',\n",
    " bootstrap = False)\n",
    "cross_val_score(clf_rand, X_train,y_train.values.ravel(), cv=3, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below is the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators= 100)\n",
    "cross_val_score(clf2, X_train,y_train.values.ravel(), cv=3, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the classifier with the hyper parameters tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [5, 21, 37, 53, 69, 85, 101, 117, 133, 150], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], 'max_leaf_nodes': [2, 5, 10], 'min_samples_leaf': [1, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# n_estimators =  10, max_leaf_nodes = 4, max_depth = None, random_state = 2,\n",
    "#                    min_samples_split =  5\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(5, 150, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "max_leaf_nodes = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   38.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                                    random_state=None,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        'max_leaf_nodes': [2, 5, 10],\n",
       "                                        'min_samples_leaf': [1, 5, 10],\n",
       "                                        'n_estimators': [5, 21, 37, 53, 69, 85,\n",
       "                                                         101, 117, 133, 150]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "gb = ensemble.GradientBoostingClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                               n_iter = 20, cv = 3, verbose=1, random_state=42,n_jobs = -1, scoring = 'f1_weighted')\n",
    "\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 21,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_leaf_nodes': 10,\n",
       " 'max_depth': 40}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 4.4350547 ,  3.29407748,  1.76290504, 12.42242233, 20.6821808 ,\n",
       "        13.28680436,  0.22645156,  4.55982304, 12.39670126,  8.35699375,\n",
       "        20.31031036, 13.13265204,  0.79666193,  3.9857192 ,  9.66266418,\n",
       "         5.88176799,  4.39024623,  3.0557371 ,  9.32074372,  6.14844076]),\n",
       " 'std_fit_time': array([0.02349409, 0.01928056, 0.01209299, 0.05304162, 0.02477188,\n",
       "        0.08143524, 0.0030735 , 0.01699617, 0.033125  , 0.01790237,\n",
       "        1.08735472, 0.05722286, 0.01490909, 0.02261185, 0.03392642,\n",
       "        0.0175904 , 0.04200916, 0.04692923, 0.02411914, 0.47221721]),\n",
       " 'mean_score_time': array([0.3949484 , 0.17895921, 0.16974338, 0.95647033, 0.96400277,\n",
       "        0.55267946, 0.06304312, 0.73580678, 0.93178217, 0.65168659,\n",
       "        0.70117664, 0.57703455, 0.06693737, 0.7242581 , 0.79485337,\n",
       "        0.97647039, 0.35976839, 0.2545499 , 0.45048936, 0.52248502]),\n",
       " 'std_score_time': array([0.02771499, 0.00058732, 0.00072194, 0.01210556, 0.04219614,\n",
       "        0.00685463, 0.00085443, 0.01271361, 0.02463622, 0.0136059 ,\n",
       "        0.02143647, 0.01746303, 0.00083032, 0.05526873, 0.01488504,\n",
       "        0.07059267, 0.00367799, 0.0040602 , 0.00127808, 0.02891749]),\n",
       " 'param_n_estimators': masked_array(data=[53, 21, 21, 150, 133, 85, 5, 117, 150, 101, 150, 85, 5,\n",
       "                    101, 117, 150, 53, 37, 101, 117],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 1, 10, 1, 10, 10, 5, 5, 10, 10, 10, 10, 5, 1, 5, 10,\n",
       "                    10, 1, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_leaf_nodes': masked_array(data=[5, 10, 5, 5, 10, 10, 2, 2, 5, 5, 10, 10, 10, 2, 5, 2,\n",
       "                    5, 5, 10, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[30, 40, 60, 40, 10, 60, 40, 20, 10, 60, 40, 50, 10, 20,\n",
       "                    100, 90, 100, None, 10, 90],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 53,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 30},\n",
       "  {'n_estimators': 21,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 40},\n",
       "  {'n_estimators': 21,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 60},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 40},\n",
       "  {'n_estimators': 133,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 10},\n",
       "  {'n_estimators': 85,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 60},\n",
       "  {'n_estimators': 5,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 2,\n",
       "   'max_depth': 40},\n",
       "  {'n_estimators': 117,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 2,\n",
       "   'max_depth': 20},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 10},\n",
       "  {'n_estimators': 101,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 60},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 40},\n",
       "  {'n_estimators': 85,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 50},\n",
       "  {'n_estimators': 5,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 10},\n",
       "  {'n_estimators': 101,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_leaf_nodes': 2,\n",
       "   'max_depth': 20},\n",
       "  {'n_estimators': 117,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 100},\n",
       "  {'n_estimators': 150,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 2,\n",
       "   'max_depth': 90},\n",
       "  {'n_estimators': 53,\n",
       "   'min_samples_leaf': 10,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 100},\n",
       "  {'n_estimators': 37,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': None},\n",
       "  {'n_estimators': 101,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 10,\n",
       "   'max_depth': 10},\n",
       "  {'n_estimators': 117,\n",
       "   'min_samples_leaf': 5,\n",
       "   'max_leaf_nodes': 5,\n",
       "   'max_depth': 90}],\n",
       " 'split0_test_score': array([0.0659978 , 0.20915602, 0.09035722, 0.11311632, 0.19858798,\n",
       "        0.18766583, 0.04844733, 0.02580435, 0.12152881, 0.10201357,\n",
       "        0.19300298, 0.21005309, 0.17996043, 0.02580435, 0.09683907,\n",
       "        0.02580435, 0.11195335, 0.08661919, 0.20058632, 0.10909565]),\n",
       " 'split1_test_score': array([0.10194431, 0.19560595, 0.11279967, 0.1042543 , 0.20082315,\n",
       "        0.20781878, 0.06883934, 0.02580607, 0.10667004, 0.11072546,\n",
       "        0.19015567, 0.20126948, 0.16243713, 0.02580607, 0.10281903,\n",
       "        0.02580607, 0.09328767, 0.12386479, 0.17761035, 0.11148275]),\n",
       " 'split2_test_score': array([0.09178169, 0.20282003, 0.07215921, 0.08426523, 0.18706152,\n",
       "        0.19024133, 0.04727042, 0.02580607, 0.09676953, 0.10858214,\n",
       "        0.18925919, 0.18311695, 0.20904664, 0.02580607, 0.10519196,\n",
       "        0.02580607, 0.10929338, 0.11659756, 0.19322953, 0.09448737]),\n",
       " 'mean_test_score': array([0.0865746 , 0.20252733, 0.09177203, 0.10054528, 0.19549088,\n",
       "        0.19524198, 0.05485236, 0.0258055 , 0.10832279, 0.10710706,\n",
       "        0.19080595, 0.19814651, 0.18381473, 0.0258055 , 0.10161669,\n",
       "        0.0258055 , 0.1048448 , 0.10902718, 0.1904754 , 0.10502192]),\n",
       " 'std_test_score': array([1.51299543e-02, 5.53565930e-03, 1.66215347e-02, 1.20668664e-02,\n",
       "        6.02990328e-03, 8.95507969e-03, 9.90195255e-03, 8.13405592e-07,\n",
       "        1.01752690e-02, 3.70640426e-03, 1.59606634e-03, 1.12161641e-02,\n",
       "        1.92224423e-02, 8.13405592e-07, 3.51444058e-03, 8.13405592e-07,\n",
       "        8.24395853e-03, 1.61202099e-02, 9.57993305e-03, 7.51253079e-03]),\n",
       " 'rank_test_score': array([16,  1, 15, 14,  3,  4, 17, 18,  9, 10,  5,  2,  7, 18, 13, 18, 12,\n",
       "         8,  6, 11], dtype=int32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68661185, 0.68093614, 0.68303433])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gb_random.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(n_estimators= 50)\n",
    "\n",
    "cross_val_score(clf2, X_train,y_train.values.ravel(), cv=3, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_random = pd.DataFrame(gb_random.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_random.to_csv('gb_random.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output from the matrix creted by random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-16ef68e58934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m  max_depth = 20)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f1_weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1245\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(n_estimators= 20,\n",
    " min_samples_leaf = 5,\n",
    " max_leaf_nodes =2,\n",
    " max_depth = 20)\n",
    "\n",
    "clf2.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "cross_val_score(clf, X_train,y_train.values.ravel(), cv=3, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_boosting = pd.DataFrame(clf2.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_boosting.to_csv('rand_boosting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is the standard classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier(n_estimators =  50)\n",
    "clf.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "#cross_val_score(clf, X_train,y_train.values.ravel(), cv=3, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_boosting = pd.DataFrame(cross_val_score.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_boosting.to_csv('std_boosting.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNYDAoTWNpFm6+qM5M06wTo",
   "collapsed_sections": [],
   "name": "Ensemble project play.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
